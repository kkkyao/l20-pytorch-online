#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Baseline (Conv1D) on MNIST-1D implemented in PyTorch with wall-clock logging.

Alignment:
  - Use the fixed split generated by preprocess:
        artifacts/preprocess/seed_{data_seed}/split.json
  - Use the same per-position normalization statistics:
        artifacts/preprocess/seed_{data_seed}/norm.json
  - Input is unified to [N, 40, 1] in NumPy, then converted to
    [N, 1, 40] for PyTorch Conv1d.
  - Log two kinds of curves:
        (1) Iteration / epoch vs performance
        (2) Wall-clock time (seconds) vs performance

Optionally, results can also be logged to Weights & Biases (wandb).
"""

import argparse
import csv
import json
import time
from pathlib import Path

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader

from data_mnist1d import load_mnist1d


# ---------------------- Data utilities ----------------------
def to_N40(x: np.ndarray, length: int = 40) -> np.ndarray:
    x = np.asarray(x)
    if x.ndim == 2 and x.shape[1] == length:
        return x
    if x.ndim == 3:
        if x.shape[1] == length and x.shape[2] == 1:
            return x[:, :, 0]
        if x.shape[1] == 1 and x.shape[2] == length:
            return x[:, 0, :]
    raise AssertionError(f"Unexpected x shape: {x.shape}")


def apply_norm_np(x: np.ndarray, mean: np.ndarray, std: np.ndarray) -> np.ndarray:
    return (x - mean[None, :]) / std[None, :]


def load_artifacts(preprocess_dir: Path, data_seed: int):
    pdir = preprocess_dir / f"seed_{data_seed}"
    with open(pdir / "split.json", "r") as f:
        split = json.load(f)
    with open(pdir / "norm.json", "r") as f:
        norm = json.load(f)
    return split, norm


# ---------------------- Model ----------------------
class Conv1DMNIST1D(nn.Module):
    def __init__(self, input_len: int = 40):
        super().__init__()
        self.conv1 = nn.Conv1d(1, 32, kernel_size=5, padding=2)
        self.relu = nn.ReLU(inplace=True)
        self.pool = nn.MaxPool1d(2)
        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, padding=2)
        self.gap = nn.AdaptiveAvgPool1d(1)
        self.fc1 = nn.Linear(64, 64)
        self.fc2 = nn.Linear(64, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = self.relu(x)
        x = self.pool(x)
        x = self.conv2(x)
        x = self.relu(x)
        x = self.gap(x)
        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        x = self.relu(x)
        return self.fc2(x)


# ---------------------- Logging helpers ----------------------
class TimeLogger:
    """
    epoch, elapsed_sec,
    train_loss, val_loss, train_acc, val_acc,
    test_loss, test_acc
    """

    def __init__(self, out_csv: Path):
        self.out_csv = out_csv
        self.start_time = None

    def on_train_begin(self):
        self.start_time = time.time()

    def on_epoch_end(
        self,
        epoch,
        train_loss,
        val_loss,
        train_acc,
        val_acc,
        test_loss,
        test_acc,
    ):
        elapsed = time.time() - self.start_time
        rec = {
            "epoch": epoch + 1,
            "elapsed_sec": elapsed,
            "train_loss": train_loss,
            "val_loss": val_loss,
            "train_acc": train_acc,
            "val_acc": val_acc,
            "test_loss": test_loss,
            "test_acc": test_acc,
        }
        write_header = not self.out_csv.exists()
        with open(self.out_csv, "a", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=rec.keys())
            if write_header:
                writer.writeheader()
            writer.writerow(rec)


class BatchLossLogger:
    def __init__(self, csv_path: Path, flush_every: int = 200):
        self.csv_path = csv_path
        with open(self.csv_path, "w") as f:
            f.write("iter,epoch,loss\n")
        self.iter = 0
        self.epoch = 0
        self.rows = []
        self.flush_every = flush_every

    def on_epoch_begin(self, epoch: int):
        self.epoch = epoch

    def on_train_batch_end(self, loss_value: float):
        self.rows.append([self.iter, self.epoch, loss_value])
        self.iter += 1
        if len(self.rows) >= self.flush_every:
            self._flush()

    def on_train_end(self):
        self._flush()

    def _flush(self):
        with open(self.csv_path, "a") as f:
            for r in self.rows:
                f.write(",".join(map(str, r)) + "\n")
        self.rows.clear()


class CSVLogger:
    """
    epoch, accuracy, loss, val_accuracy, val_loss, test_accuracy, test_loss
    """

    def __init__(self, csv_path: Path):
        self.csv_path = csv_path
        self.initialized = False

    def log_epoch(
        self,
        epoch,
        train_loss,
        val_loss,
        train_acc,
        val_acc,
        test_loss,
        test_acc,
    ):
        rec = {
            "epoch": epoch,
            "accuracy": train_acc,
            "loss": train_loss,
            "val_accuracy": val_acc,
            "val_loss": val_loss,
            "test_accuracy": test_acc,
            "test_loss": test_loss,
        }
        write_header = not self.initialized
        with open(self.csv_path, "a", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=rec.keys())
            if write_header:
                writer.writeheader()
            writer.writerow(rec)
        self.initialized = True


# ---------------------- Training helpers ----------------------
def train_one_epoch(model, loader, criterion, optimizer, device, batch_logger=None):
    model.train()
    loss_sum, correct, total = 0.0, 0, 0
    for x, y in loader:
        x, y = x.to(device), y.to(device)
        optimizer.zero_grad()
        logits = model(x)
        loss = criterion(logits, y)
        loss.backward()
        optimizer.step()

        if batch_logger:
            batch_logger.on_train_batch_end(loss.item())

        loss_sum += loss.item() * y.size(0)
        correct += (logits.argmax(1) == y).sum().item()
        total += y.size(0)

    return loss_sum / total, correct / total


def evaluate(model, loader, criterion, device):
    model.eval()
    loss_sum, correct, total = 0.0, 0, 0
    with torch.no_grad():
        for x, y in loader:
            x, y = x.to(device), y.to(device)
            logits = model(x)
            loss = criterion(logits, y)
            loss_sum += loss.item() * y.size(0)
            correct += (logits.argmax(1) == y).sum().item()
            total += y.size(0)
    return loss_sum / total, correct / total


# ---------------------- Main ----------------------
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--opt", choices=["sgd", "adam", "rmsprop"], default="sgd")
    ap.add_argument("--seed", type=int, default=0)
    ap.add_argument("--data_seed", type=int, default=42)
    ap.add_argument("--epochs", type=int, default=140)
    ap.add_argument("--bs", type=int, default=128)
    ap.add_argument("--preprocess_dir", type=str, default="artifacts/preprocess")
    ap.add_argument("--wandb", action="store_true")
    ap.add_argument("--wandb_project", type=str, default="mnist1d_conv1d_baselines")
    ap.add_argument("--wandb_entity", type=str, default=None)
    ap.add_argument("--wandb_group", type=str, default=None)
    args = ap.parse_args()

    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(args.seed)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    (xtr, ytr), (xte, yte) = load_mnist1d(length=40, seed=args.data_seed)
    xtr = to_N40(xtr).astype(np.float32)
    xte = to_N40(xte).astype(np.float32)

    split, norm = load_artifacts(Path(args.preprocess_dir), args.data_seed)
    mean, std = np.array(norm["mean"]), np.array(norm["std"])

    x_train = apply_norm_np(xtr[split["train_idx"]], mean, std)[..., None]
    x_val = apply_norm_np(xtr[split["val_idx"]], mean, std)[..., None]
    x_test = apply_norm_np(xte, mean, std)[..., None]

    def to_tensor(x): return torch.from_numpy(x).permute(0, 2, 1)

    train_loader = DataLoader(
        TensorDataset(to_tensor(x_train), torch.from_numpy(ytr[split["train_idx"]])),
        batch_size=args.bs, shuffle=True
    )
    val_loader = DataLoader(
        TensorDataset(to_tensor(x_val), torch.from_numpy(ytr[split["val_idx"]])),
        batch_size=args.bs
    )
    test_loader = DataLoader(
        TensorDataset(to_tensor(x_test), torch.from_numpy(yte)),
        batch_size=args.bs
    )

    model = Conv1DMNIST1D().to(device)
    if args.opt == "sgd":
        optimizer = optim.SGD(model.parameters(), lr=0.1)
    elif args.opt == "adam":
        optimizer = optim.Adam(model.parameters(), lr=1e-3)
    else:
        optimizer = optim.RMSprop(model.parameters(), lr=1e-3)

    criterion = nn.CrossEntropyLoss()

    run_dir = Path("runs") / f"baseline_conv1d_{args.opt}_seed{args.seed}"
    run_dir.mkdir(parents=True, exist_ok=True)

    time_logger = TimeLogger(run_dir / "time_log.csv")
    batch_logger = BatchLossLogger(run_dir / "curve.csv")
    csv_logger = CSVLogger(run_dir / "train_log.csv")

    wandb_run = None
    if args.wandb:
        import wandb
        wandb_run = wandb.init(
            project=args.wandb_project,
            entity=args.wandb_entity,
            group=args.wandb_group,
            name=f"baseline_{args.opt}_seed{args.seed}",
        )

    time_logger.on_train_begin()

    for epoch in range(args.epochs):
        batch_logger.on_epoch_begin(epoch)

        train_loss, train_acc = train_one_epoch(
            model, train_loader, criterion, optimizer, device, batch_logger
        )
        val_loss, val_acc = evaluate(model, val_loader, criterion, device)
        test_loss, test_acc = evaluate(model, test_loader, criterion, device)

        time_logger.on_epoch_end(
            epoch,
            train_loss, val_loss, train_acc, val_acc,
            test_loss, test_acc,
        )
        csv_logger.log_epoch(
            epoch,
            train_loss, val_loss, train_acc, val_acc,
            test_loss, test_acc,
        )

        if wandb_run:
            import wandb
            wandb.log({
                "epoch": epoch,
                "train_loss": train_loss,
                "train_acc": train_acc,
                "val_loss": val_loss,
                "val_acc": val_acc,
                "test_loss": test_loss,
                "test_acc": test_acc,
            })

    batch_logger.on_train_end()

    # -------- final test (unchanged semantics) --------
    final_test_loss, final_test_acc = evaluate(model, test_loader, criterion, device)

    with open(run_dir / "result.json", "w") as f:
        json.dump({
            "test_loss": final_test_loss,
            "test_acc": final_test_acc,
        }, f, indent=2)

    if wandb_run:
        import wandb
        wandb.log({
            "final_test_loss": final_test_loss,
            "final_test_acc": final_test_acc,
        })
        wandb.finish()


if __name__ == "__main__":
    main()
